{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbbfbec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from app.core import chunking as ch\n",
    "importlib.reload(ch)\n",
    "from app.core.chunking import tokenize_and_chunk, validate_overlap\n",
    "text = \"Cümle 1. Cümle 2. Cümle 3. Cümle 4. Cümle 5. \" * 40\n",
    "chs = tokenize_and_chunk(text, max_tokens=120, overlap=30, min_chunk_tokens=20, store_tokens=True)\n",
    "len(chs), validate_overlap(chs, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48893780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UYUMSUZ: c1 c2\n"
     ]
    }
   ],
   "source": [
    "from app.core.chunking import _get_tokenizer\n",
    "tok = _get_tokenizer()\n",
    "for a,b in zip(chs, chs[1:]):\n",
    "    ta = tok.encode(a['text'])\n",
    "    tb = tok.encode(b['text'])\n",
    "    if len(ta) >=30 and len(tb) >=30 and ta[-30:] != tb[:30]:\n",
    "        print('UYUMSUZ:', a['id'], b['id'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "153dd6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_sentence = \"A \" * 1200  # tek dev cümle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63b3d0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(c['token_count'] <= 300 for c in chs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "939914e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(c['token_count'] for c in chs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e9e947d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chs2 = tokenize_and_chunk(long_sentence, max_tokens=300, overlap=50, min_chunk_tokens=20)\n",
    "all(c['token_count'] <= 300 for c in chs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e941ed09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"Bir. İki. Üç. Dört. Beş. \" * 10\n",
    "no_overlap = tokenize_and_chunk(sample, max_tokens=120, overlap=0)\n",
    "with_overlap = tokenize_and_chunk(sample, max_tokens=120, overlap=40)\n",
    "len(no_overlap), len(with_overlap)  # with_overlap genelde biraz daha fazla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f467283d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shorty = \"A. B. C. D. E.\"\n",
    "chs = tokenize_and_chunk(shorty, max_tokens=50, overlap=0, min_chunk_tokens=20)\n",
    "# Sonuç 0 veya 1 küçük chunk (çoğu zaman 0) => min_chunk_tokens filtresi\n",
    "chs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "530f9563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 361, 1.74)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app.core.ingestion import basic_text_stats\n",
    "from app.core.chunking import _get_tokenizer\n",
    "txt = \"Bu bir test cümlesidir. \" * 40\n",
    "stats = basic_text_stats(txt)\n",
    "real_tokens = len(_get_tokenizer().encode(txt))\n",
    "(stats['approx_tokens'], real_tokens, round(real_tokens / max(stats['approx_tokens'],1),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb4c4ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app.core.chunking import tokenize_and_chunk\n",
    "from app.core.embeddings import get_or_compute_embeddings\n",
    "text = \"Model farkı testi için örnek metin. \" * 10\n",
    "chs = tokenize_and_chunk(text)\n",
    "e1 = get_or_compute_embeddings(chs, model='text-embedding-004')\n",
    "e2 = get_or_compute_embeddings(chs, model='text-embedding-004-alt')\n",
    "len(e1), len(e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "611d2d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = \"A B C D E F G\"\n",
    "t2 = \"A B C D E F G H\"\n",
    "chs1 = [{'id':'c1','text':t1,'token_count':1,'start_token':0,'end_token':0}]\n",
    "chs2 = [{'id':'c1','text':t2,'token_count':1,'start_token':0,'end_token':0}]\n",
    "from app.core.embeddings import _hash_key\n",
    "_hash_key('text-embedding-004', t1) != _hash_key('text-embedding-004', t2)  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93f29468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from app.core.embeddings import get_or_compute_embeddings\n",
    "chs = [{'id':'c1','text':'Deterministik test','token_count':10,'start_token':0,'end_token':9}]\n",
    "a = get_or_compute_embeddings(chs)[0]['embedding']\n",
    "b = get_or_compute_embeddings(chs)[0]['embedding']\n",
    "a == b  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef048a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "570c5fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "from app.core.chunking import tokenize_and_chunk\n",
    "from app.core.embeddings import get_or_compute_embeddings\n",
    "text = \"Test cümlesi. \" * 50\n",
    "chs = tokenize_and_chunk(text, max_tokens=200, overlap=30)\n",
    "assert chs, 'Chunk üretilmedi'\n",
    "for c in chs: assert c['token_count'] <= 200\n",
    "emb = get_or_compute_embeddings(chs)\n",
    "assert len(emb) == len(chs)\n",
    "print('OK')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
