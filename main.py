"""Streamlit uygulamasÄ± giriÅŸ noktasÄ± (main).
Step 1: PDF/TXT Upload & Extraction UI
Ã‡alÄ±ÅŸtÄ±rma: streamlit run main.py
"""

import streamlit as st
from app.core import ingestion
from app.core.chunking import tokenize_and_chunk
from app.core.embeddings import get_or_compute_embeddings

st.set_page_config(page_title="AI Teaching Assistant", layout="wide")

st.title("ğŸ“˜ Ders Materyali YÃ¼kleme")
st.caption("AdÄ±m 1: PDF veya TXT materyalini yÃ¼kle, metni Ã§Ä±kar ve onayla.")

if "source_text" in st.session_state:
    st.success("Bir materyal yÃ¼klendi. Yeniden yÃ¼klemek istersen aÅŸaÄŸÄ±dan devam edebilirsin.")

uploaded = st.file_uploader("Materyal seÃ§ (PDF/TXT)", type=["pdf","txt"], accept_multiple_files=False)

MAX_MB = 5
if uploaded:
    size_mb = uploaded.size / (1024 * 1024)
    if size_mb > MAX_MB:
        st.error(f"Dosya {size_mb:.2f} MB (> {MAX_MB} MB limit). Demo sÃ¼rÃ¼m limiti aÅŸÄ±ldÄ±.")
    else:
        raw_text = ""
        if uploaded.type == "application/pdf" or uploaded.name.lower().endswith(".pdf"):
            raw_text = ingestion.read_pdf(uploaded.read())
        else:
            raw_text = ingestion.read_txt(uploaded.read())

        normalized = ingestion.normalize_text(raw_text)
        stats = ingestion.basic_text_stats(normalized)

        if not normalized.strip():
            st.error("Metin Ã§Ä±karÄ±lamadÄ± (boÅŸ veya okunamadÄ±). FarklÄ± bir dosya deneyin.")
        else:
            if stats["chars"] < 300:
                st.warning("Metin Ã§ok kÄ±sa; analiz kalitesi dÃ¼ÅŸÃ¼k olabilir.")

            with st.expander("Ã–nizleme (ilk 800 karakter)"):
                st.text(normalized[:800])
            st.write(
                f"**Karakter:** {stats['chars']} | **Kelime:** {stats['words']} | **YaklaÅŸÄ±k Token:** {stats['approx_tokens']} | **SatÄ±r:** {stats['lines']}"
            )
            confirm = st.button("Metni kabul et ve devam et", type="primary")
            if confirm:
                st.session_state["source_text"] = normalized
                st.session_state["source_meta"] = {
                    "filename": uploaded.name,
                    "size_mb": size_mb,
                    "stats": stats,
                }
                st.success("Materyal kaydedildi. Sonraki adÄ±m: Chunking & Embeddings (hazÄ±rlanacak).")

st.divider()
st.write("â­ Sonraki gelecek adÄ±mlar: Chunk oluÅŸturma, Embeddings ve Coverage analizi.")

st.divider()
st.header("ğŸ”— AdÄ±m 2: Chunking & Embeddings")
if 'source_text' not in st.session_state:
    st.info("Ã–nce AdÄ±m 1'de materyal yÃ¼kleyip onayla.")
else:
    col1, col2, col3 = st.columns([1,1,1])
    with col1:
        max_tokens = st.number_input("Max tokens", min_value=100, max_value=1200, value=450, step=50)
    with col2:
        overlap = st.number_input("Overlap tokens", min_value=0, max_value=400, value=50, step=10)
    with col3:
        min_chunk_tokens = st.number_input("Min chunk tokens", min_value=5, max_value=200, value=20, step=5)

    if st.button("Chunk OluÅŸtur", type="primary"):
        chunks = tokenize_and_chunk(
            st.session_state['source_text'],
            max_tokens=max_tokens,
            overlap=overlap,
            min_chunk_tokens=min_chunk_tokens,
        )
        st.session_state['chunks'] = chunks
        st.success(f"{len(chunks)} chunk Ã¼retildi.")

    if 'chunks' in st.session_state:
        show = st.checkbox("Chunk Ã¶nizleme (ilk 5)", value=True)
        if show:
            for ch in st.session_state['chunks'][:5]:
                st.code(f"{ch['id']} | tokens={ch['token_count']}\n" + ch['text'][:300] + ('...' if len(ch['text'])>300 else ''))
        if st.button("Embeddings Hesapla (placeholder)"):
            with st.spinner("Embedding hesaplanÄ±yor / cache kontrol ediliyor..."):
                embedded = get_or_compute_embeddings(st.session_state['chunks'], model='text-embedding-004', use_real=False)
                st.session_state['embedded_chunks'] = embedded
            st.success("Embeddings hazÄ±r (fake). GerÃ§ek API entegrasyonu TODO.")
    if 'embedded_chunks' in st.session_state:
        emb_list = st.session_state['embedded_chunks']
        if not emb_list:
            st.warning("Embedding Ã¼retilemedi (boÅŸ iÃ§erik veya tÃ¼m chunk'lar elendi).")
        else:
            first = emb_list[0].get('embedding') or []
            st.write(f"Ã–rnek vektÃ¶r boyutu: {len(first)} (placeholder)")
            st.info("Coverage skoru iÃ§in bir sonraki aÅŸamada hedef iÃ§erik karÅŸÄ±laÅŸtÄ±rmasÄ± eklenecek.")

st.divider()
st.header("ğŸ“Š AdÄ±m 3: Coverage Analizi")
if 'embedded_chunks' not in st.session_state or not st.session_state.get('embedded_chunks'):
    st.info("Ã–nce AdÄ±m 2'de chunk ve embedding Ã¼ret.")
else:
    st.caption("Her satÄ±r bir konu / baÅŸlÄ±k. BoÅŸ satÄ±rlar atlanÄ±r.")
    default_topics = """GiriÅŸ
TanÄ±mlar
Ã–rnekler
Uygulama AlanlarÄ±
SonuÃ§""".strip()
    topics_text = st.text_area("Konu Listesi", value=default_topics, height=160)
    cthr, pthr = st.columns(2)
    with cthr:
        covered_thr = st.slider("Covered EÅŸiÄŸi", 0.5, 0.95, 0.78, 0.01)
    with pthr:
        partial_thr = st.slider("Partial EÅŸiÄŸi", 0.3, 0.9, 0.60, 0.01)
    if st.button("Coverage Hesapla"):
        from app.core.coverage import compute_coverage
        with st.spinner("Coverage hesaplanÄ±yor..."):
            cov = compute_coverage(
                st.session_state['embedded_chunks'],
                topics_text,
                covered_thr=covered_thr,
                partial_thr=partial_thr,
                model='text-embedding-004',
                use_real=False,
            )
            st.session_state['coverage'] = cov
        st.success("Coverage hesaplandÄ±.")
    if 'coverage' in st.session_state:
        cov = st.session_state['coverage']
        summ = cov['summary']
        st.subheader("Ã–zet")
        st.write(f"Covered: {summ['covered']} | Partial: {summ['partial']} | Missing: {summ['missing']} | Coverage Ratio: {summ['coverage_ratio']:.2f}")
        st.subheader("Detay")
        import pandas as pd
        df = pd.DataFrame(cov['topics'])
        st.dataframe(df, use_container_width=True)
